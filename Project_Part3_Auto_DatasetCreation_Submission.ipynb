{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DOMAIN: Automobile\n",
    "- CONTEXT: A brand research company wants to understand which cars or car manufacturers are popular in a certain area of the city or locality. Company has a team which takes pictures of the cars randomly through the day. Using this the company wants to set up an automation which can classify the make of the car once the picture has been given as an input.\n",
    "- TASK: Help to build the image dataset to be used by the AI team to build an image classifier data. Import and display the images in python against their labels. Comment on the challenges faced during this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assumptions:\n",
    "\n",
    "   - Company resources will be taking random images of cars in the city, these images will be used as input on a model which will predict the class and do the needful on reporting it.\n",
    "   - AI team will create a image classifier model, using a image data set for cars. These images of cars could be from any make, having any background, colors, type (sedan,sux,coup etc.) and many other variations.\n",
    "   - Our job is to create a data set of images with related label/labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Image Data set creation strategy\n",
    "- Using API's to capture images and related tags\n",
    "    - We have used image search engines/API (Google/Bing/flicker/pinetrest) to capture images based on our searches and download them as per our need\n",
    "    - manual intervention is required for random or exaustive evaluation of data being kept for traing. A good model highly dependes on kind of data it got trained on.\n",
    "    - Can use data augmention to create more data from limited images - this step can be done during model creation as well\n",
    "    - Organize the images based on car's brand/type on folders or in a data set.\n",
    "    - Import and visualize if the captured and validated images are exactly what you are looking for as train data or need more intervention to improve it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challanges Faced during dataset creation\n",
    "\n",
    "    - A model can work best if it was trained on a better training data.\n",
    "    - Few option avaialbe to us was manually take each image/download images/crowd source it with pinetrest or other similar apps\n",
    "    - With our approach of web scrapping using API, google image search has a limit of 300 images or so, we have to tweak the search parameters all the time to get diff immages if we have to capture huge number of non repetable images. \n",
    "    - Even with that there is no gurantee we will get duplicate or redundant images. We have to manually navigate and delete or edit those.\n",
    "    - One major issue with web scrapping is we might get many images which are full of noisy text and watermarks\n",
    "    - Multiple cases we will find we have more than one brands or mltiple models of same brand in single image.\n",
    "    - Legality of web-scraping images, we need to read the fine print of usage for many images if we can use it for academic or buisness purpose. Most of the cases academmic is acceptable but buisness usage of the images need respective agreement/contracts.\n",
    "    - While searching we will also get many diff formats like .gifs/.jpegs, they behave differently while using in python\n",
    "    - Lot of images have watermarks, edited using photoshop, few concept cars designs or sketch drawings will also landup in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importinng necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib.request\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "plt.rcParams['figure.figsize'] = [16, 10]\n",
    "plt.rcParams['font.size'] = 16\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "from keras.preprocessing import image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Function for downloading Image\n",
    "    - Using selenium for image scraping\n",
    "    - Firefox driver is used to invoke firfox browser and search on google with respective keywords provided\n",
    "    - Selenium will be used to scroll and load more images as needed, capture the path of the images\n",
    "    - urllib will be used to download the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    \n",
    "def downloadImages(searchKeyword, folderPath, folderName):\n",
    "    print(\"Images to download:\",searchKeyword)\n",
    "    site = 'https://www.google.com/search?tbm=isch&q='+searchKeyword\n",
    "    myPath=os.path.join(folderPath, folderName)\n",
    "    if not os.path.isdir(myPath):\n",
    "        os.makedirs(myPath)\n",
    "    \n",
    "    driver = webdriver.Firefox(executable_path = 'F:\\Webdrivers\\geckodriver.exe')\n",
    "\n",
    "    driver.get(site)\n",
    "\n",
    "#increase the number of scrolls for more images\n",
    "    i = 0\n",
    "\n",
    "    while i<1:  \n",
    "        driver.execute_script(\"window.scrollBy(0,document.body.scrollHeight)\")\n",
    "\n",
    "        try:\n",
    "            driver.find_element_by_xpath(\"/html/body/div[2]/c-wiz/div[3]/div[1]/div/div/div/div/div[5]/input\").click()\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        time.sleep(5)\n",
    "        i+=1\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "    img_tags = soup.find_all(\"img\", class_=\"rg_i\")\n",
    "\n",
    "\n",
    "    count = 0\n",
    "    for i in img_tags:\n",
    "        try:\n",
    "            fullfilename = os.path.join(myPath, str(count)+\".jpg\")\n",
    "\n",
    "            urllib.request.urlretrieve(i['src'], fullfilename)\n",
    "            count+=1\n",
    "            print(\"Number of images downloaded = \"+str(count),end='\\r')\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    print(\"Number of images downloaded = \"+str(count))\n",
    "    print(\"Downloaded Images are saved in : \", myPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating data set with search keywords and folder name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [['car alfa romeo', 'AlfaRomeo'],\n",
    "        ['car Audi', 'Audi'],\n",
    "        ['car BMW', 'BMW'],\n",
    "        ['car Bentley', 'Bentley'],\n",
    "#        ['car Buick', 'Buick'],\n",
    "#        ['car Cadillac', 'Cadillac'],\n",
    "#        ['car Chevrolet', 'Chevrolet'],\n",
    "        ['car Honda', 'Honda'],\n",
    "        ['car Hyundai', 'Hyundai'],\n",
    "        ['car Toyota', 'Toyota'],\n",
    "        ['car Tesla', 'Tesla'],\n",
    "        ['car Maruti Suzuki', 'MarutiSuzuki'],\n",
    "        ['car Tata', 'Tata']        \n",
    "       ] \n",
    "\n",
    "#reduced the data set for reducing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data, columns = ['Search', 'FolderName']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetPath='F:\\GreatLearning\\AI\\ComputerVision\\Project\\Project_dataSet_Creation_Cars'\n",
    "downloadImages('car Audi',datasetPath ,'Audi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apply(lambda row : downloadImages(row['Search'],datasetPath,row['FolderName']),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adding another a catgory of cars like 'SUV'/'Hatchback' to see the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "downloadImages('car Hatchback',datasetPath ,'Hatchback')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rootdir='F:\\GreatLearning\\AI\\ComputerVision\\Project\\Project_dataSet_Creation_Cars'\n",
    "for file in os.listdir(rootdir):\n",
    "    d = os.path.join(rootdir, file)\n",
    "    if os.path.isdir(d):\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in os.listdir(rootdir):\n",
    " #   d = os.path.join(rootdir, file)\n",
    "  #  if os.path.isdir(d):\n",
    "   #     print(file)\n",
    "        \n",
    "        \n",
    "        \n",
    "listOfCarBrands= [ file for file in os.listdir(rootdir) if os.path.isdir(os.path.join(rootdir, file))]\n",
    "print(listOfCarBrands)\n",
    "NUM_CATEGORIES = len(listOfCarBrands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for carName in listOfCarBrands:\n",
    "    print('{} {} images'.format(carName, len(os.listdir(os.path.join(rootdir, carName)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "for category_id, category in enumerate(listOfCarBrands): \n",
    "    for file in os.listdir(os.path.join(rootdir, category)): \n",
    "        train.append(['{}/{}/{}'.format(rootdir, category, file), category_id, category]) \n",
    "        \n",
    "train = pd.DataFrame(train, columns = ['file', 'category_id', 'category'])\n",
    "print(train.head(5))\n",
    "train.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img(filepath, size):\n",
    "    img = image.load_img(os.path.join(rootdir, filepath), target_size = size)\n",
    "    img = image.img_to_array(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using matplotlib for this\n",
    "\n",
    "fig = plt.figure(1, figsize=(NUM_CATEGORIES, NUM_CATEGORIES)) \n",
    "grid = ImageGrid(fig, 111, nrows_ncols=(NUM_CATEGORIES, NUM_CATEGORIES), axes_pad=0.05)\n",
    "i = 0\n",
    "\n",
    "\n",
    "for category_id, category in enumerate(listOfCarBrands):\n",
    "    for filepath in train[train['category'] == category]['file'].values[:NUM_CATEGORIES]:\n",
    "        ax = grid[i]\n",
    "        img = read_img(filepath, (224,224)) # read_img function call; filepath specified, img_size hard-coded\n",
    "        ax.imshow(img/255.)\n",
    "        ax.axis('off')\n",
    "        if i % NUM_CATEGORIES == NUM_CATEGORIES - 1: # Labeling the row-categories (I believe)\n",
    "            ax.text(250, 112, filepath.split('/')[1], verticalalignment='center')\n",
    "        i += 1\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reference:\n",
    "    - https://towardsdatascience.com/how-to-create-your-own-image-dataset-for-deep-learning-b53f1c22c443\n",
    "    - https://medium.com/ai%C2%B3-theory-practice-business/build-image-dataset-from-scratch-7752e9e22162\n",
    "    - https://medium.com/analytics-vidhya/create-your-own-real-image-dataset-with-python-deep-learning-b2576b63da1e\n",
    "    - https://www.pyimagesearch.com/2018/04/09/how-to-quickly-build-a-deep-learning-image-dataset/\n",
    "    - https://www.pyimagesearch.com/2017/12/04/how-to-create-a-deep-learning-dataset-using-google-images/\n",
    "    - https://dev.to/dillir07/a-python-package-with-selenium-to-download-high-res-image-using-google-search-by-image-6ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
