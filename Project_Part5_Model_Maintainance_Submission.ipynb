{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain in depth your strategy to maintain and support the AIML image classifier after it in production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Strategy Summary:\n",
    "    - Please refer below cells for detailed observations\n",
    "    - Considering our observed points we have to cover below details for a better maintanance approach and put it in pipeline process or with manual interventions:\n",
    "        - Classification and Labeling module\n",
    "            - With manual or automated approach we have to map the input images with ground truth\n",
    "            - This will help us in understanding the model performance as well as creating future training data\n",
    "            \n",
    "        - Monitoring Module:\n",
    "            - In this module we will be trackinng the model performance when we will have data available from above module.\n",
    "            - Automate the process to identify the drift in model performance for weekly or monthly basis\n",
    "            \n",
    "        - Retraining Module:\n",
    "            - Based on monitoring results we have to identify what part of model need to be trained\n",
    "            - it could be just the final classifer layer for ex. if we received new species\n",
    "            - It could be few additonal layers along with classifer\n",
    "            - Or if we came to conclusion the whole model is turining obsolete and need a whole training\n",
    "            - Identify how soon a model need to be retained\n",
    "                - We should have accumulated sufficient mix of old and new data for better model performance\n",
    "            \n",
    "        - Deploymnet:\n",
    "            - We dont want to suddenly change the model, run it in parallel with existing model for an acceptable time period and identify the performance difference\n",
    "            - If the model performance is significantly improved then its predecessor, we will deploy it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need for maintainance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Reasons for model maintainance \n",
    "- Models prediction capability fades with time if not upgraded/updated on time ( real world example - Kodak/Nokia):\n",
    "    - How deep learning outperformed traditional methods in imagenet calssification, with time it surpassed even human level\n",
    "    - need to devote to monitoring, validating, and tweaking it all to keep it uptodate\n",
    "- Appearance of new classes (probabilistic variables):\n",
    "     - CNN model can work or predict the things that it was trained to predict or to put in different words the model is only aware of the classes it was taught. If any new class came in picture it will falsly classify it to any one of the existing class.\n",
    "- Data dependencies:\n",
    "     - a model can work classify based on its data its being trained on.\n",
    "     - with time we will be receiving/discovering new data/classes/edge case scenarios/rare species which need to be added to the train data and update the classifer with proper class\n",
    "- Pipeline processes:\n",
    "    - We would have designed the model and its pipeline in a way suited best during the time of creation but with time we might have to update or change that with change in new technologies and approaches.\n",
    "    - In our data we are using jpg/png as immage source now but if we start getting data in some other image formats like raw image format from cameras. \n",
    "- Env/Configuration settings:\n",
    "     - Upgrade in softwares - like Python versions/OS/DB\n",
    "     - Upgrade in hardware - advanced new CPU/GPU/TPU\n",
    "- Change of Concept:\n",
    "    - with time and technolgy we may come across few ground breaking shifts in concept which might be the base of current model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We will try to derive our strategy keeping our Flower classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When we deploy models to production and expect to observe error rates like those we saw during model evaluation, we are making an assumption that future data will be similar to past observed data. Specifically, we are assuming that the distributions of the features and targets will remain fairly constant. But this assumption usually does not hold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Drift In Concept:\n",
    "    - Pattern of flower changes with change in climatic conditions\n",
    "    - These kind of things are very slow and take long time to come in affect, we have to expand our time horizon to a very wide range to see the affect. Ex. extinction of few flowers, few flowers are getting mutated due to climate chagnes. If we are widning our horizon to cover these extents we simply dont have enough data going back but for future we might keep track of changes.\n",
    "    - These slight drifts in concepts need to be identified and added in training phases to get better predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regular Performance Metrics Evaluation & tweakings:\n",
    "    - Like maintanance of any ship or software, we have to dedicate resource_hours for monitoring the performance of the on daily/weakly basis and review it. Of cource this can be traced only if we have knowledge of groundtruth which we might recieve may be after some time with manual intervention.\n",
    "    - Acceptance criteria could be many - cross entropy loss, accuracy. Lets assume we took accuracy as our matrics of measure. We can keep our acceptance range baseed on project/buisness owners decisions and models capacity like 75%-95%.\n",
    "    - Create acceptance criteria boundries and percentage of acceptable failures it should not cross. From above range if the average accuracy over a week is dropping towards lower boundries we need to re look into the input data its failing for and why its happening.\n",
    "    - Once the model is close to testing the boundries of failures we might be entering into a phase where the model started to show its incapabilites and need tweaking or re-training or updation or addition of new classes.\n",
    "    - In parallel to production, maintainance team has to carry out effort on re-training and finding new weights for the model which then can be updated for using the existing model. For this reason we can use keras model.save and model.save_weights to keep the two parts independent of each other and maintain them separately.\n",
    "    - If by updating the weights is not keeping us in acceptable ranges then we have re look into the model to identify what needs to be updated. It could start from our preprocessing of images, number of layers, normalization or dropout layers or the final classification Layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Monitoring a model:\n",
    "    - For Monitoring we have to introduce a step where either we randomly pick samples and manually validate it with ground truths or wait for some time till the ground truths are avialabel with data.\n",
    "    - We might have to introduce a step for cleaning and labelling the data to validate the prediction and simultaneously creating our data for future training purposes.\n",
    "    - When we look deep into model monitoring its not just the accuracy we have to look for, we have to look for which data it failed, what was predicted and what was ground truth.\n",
    "    - we have to make a proper logging mechanism which will track every prediction it has made with each input data and capture its prediction and ground truth as well. \n",
    "    - This seems easier in words but in reality we might not have actual label for a long time until it was validated or tagged or identifed manually and put into the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Retraining The model:\n",
    "    - Retraining a machine learning model that is already deployed in a live production environment is much easier said than done. There are multiple ways to solve a particular data-driven problem, and as we see more data our choice of the model may change.\n",
    "    - If a modelâ€™s predictive performance has fallen due to changes in the environment, the solution is to retrain the model on a new training set which reflects the current reality.\n",
    "    - We can consider retraining of model only after we have gathered sufficient mix of new and old data.\n",
    "    - We have to also keep in mind if we want to retrain the whole model or just part of it as training whole model might be time consuming and may not be a good idea all the time.\n",
    "    - Another point to consider is if we can deploy directly the new model with better accuracy or run it in parallel with old model to compare results with ground truth when available.\n",
    "    - \n",
    "\n",
    "- Automating the Retraining process\n",
    "    - If we assume the model selection is sustainable and just retraining the model and updating the weigts or redploying the whole model is viable we can use multiple approaches to automate the process.\n",
    "    - We can easily use Jenkins to automate the process of training and if the model is giving significantly better result then its predecessor on identifed new Validation data, it can auto trigger the deployment process as well during off hours.\n",
    "    - Triggering of training the model can be automated/manually based on accuracy metrices gathered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reference\n",
    "    - https://towardsdatascience.com/how-to-apply-continual-learning-to-your-machine-learning-models-4754adcd7f7f\n",
    "    - https://mlinproduction.com/model-retraining/#:~:text=Rather%20retraining%20simply%20refers%20to,t%20involve%20any%20code%20changes.\n",
    "    - https://machinelearningmastery.com/deploy-machine-learning-model-to-production/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
