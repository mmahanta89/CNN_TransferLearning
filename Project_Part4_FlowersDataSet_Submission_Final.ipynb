{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Importing Necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, concatenate, Input, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adam, Adadelta\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tflearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import tflearn.datasets.oxflower17 as oxflower17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initializing Image height/width/size/epochs as we are using it in multiple places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height=100\n",
    "img_width=100\n",
    "batch_size=32\n",
    "nb_epochs=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using data present in local folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring the folders containing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_immediate_subdirectories(a_dir):\n",
    "    return [name for name in os.listdir(a_dir)\n",
    "            if os.path.isdir(os.path.join(a_dir, name))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specPath='F:\\\\GreatLearning\\AI\\\\ComputerVision\\\\Project\\\\Flowers-Classification\\\\17flowers-train\\\\jpg'\n",
    "specPathTest='F:\\\\GreatLearning\\\\AI\\\\ComputerVision\\\\Project\\\\Flowers-Classification\\\\Test\\\\'\n",
    "cat_Folder_list=get_immediate_subdirectories(specPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of species in the train folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flower_species=cat_Folder_list\n",
    "print('List of Flower species: ', Flower_species)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we have 17 species of flowers tagged with numbers from 0 to 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nummber of images in the train sub folders along with categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No. of images under each plant species foler for train\n",
    "for img in Flower_species:\n",
    "    print('{}   -->   {} training images'.format(img, len(os.listdir(os.path.join(specPath, img)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are 17 species are available for training and each flower species contains 80-85 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "rootdir='F:\\\\GreatLearning\\\\AI\\\\ComputerVision\\\\Project\\\\'\n",
    "rootJPG=os.path.join(os.path.join(rootdir,'Flowers-Classification\\\\17flowers-train'),'jpg')\n",
    "os.chdir(os.path.join(specPath,Flower_species[0])) #changing current directory to open file easily\n",
    "count=0\n",
    "imgList=[]\n",
    "for file in glob.glob(\"*.jpg\"):\n",
    "    print(file)\n",
    "    count+=1\n",
    "    imgList.append(file)\n",
    "    if (count==10):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "Image.open(\"image_0002.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#capture basic details of images\n",
    "def imgBasics(path,imgName):\n",
    "    img1= os.path.join(path, imgName)\n",
    "    pic = imageio.imread(img1)\n",
    "    plt.figure(figsize = (5,5))\n",
    "    plt.imshow(pic)\n",
    "\n",
    "    #Basic properties of image\n",
    "    print('Type of the image : ' , type(pic)) \n",
    "    print('Shape of the image : {}'.format(pic.shape)) \n",
    "    print('Image Hight {}'.format(pic.shape[0])) \n",
    "    print('Image Width {}'.format(pic.shape[1])) \n",
    "    print('Dimension of Image {}'.format(pic.ndim))\n",
    "    print('Image size {}'.format(pic.size)) \n",
    "    print('Maximum RGB value in this image {}'.format(pic.max())) \n",
    "    print('Minimum RGB value in this image {}'.format(pic.min()))\n",
    "    print('Value of only R channel {}'.format(pic[ 100, 50, 0])) \n",
    "    print('Value of only G channel {}'.format(pic[ 100, 50, 1])) \n",
    "    print('Value of only B channel {}'.format(pic[ 100, 50, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstSpecies=os.path.join(specPath,Flower_species[0])\n",
    "imgBasics(firstSpecies,imgList[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgBasics(firstSpecies,imgList[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgBasics(firstSpecies,imgList[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgBasics(firstSpecies,imgList[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the above observed images, each image has same height but diff width\n",
    "- We can see the color composition are diff\n",
    "- images were taken at different light conditions\n",
    "- images were taken at different angles as well\n",
    "- not all images are center weighted, few have multiple images as well\n",
    "- For color concentratin as well, we can notice the distribution is spreaded on R,G & B. \n",
    "- Lot of baackground information is also present including the leaves or trees or presence of other objects as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir='F:\\\\GreatLearning\\\\AI\\\\ComputerVision\\\\Project\\\\'\n",
    "os.chdir(rootdir) #resetting back to original directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data from all folders along with mapped categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "images_per_class = {}\n",
    "for class_folder_name in os.listdir(specPath):\n",
    "    class_folder_path = os.path.join(specPath, class_folder_name)\n",
    "    if os.path.isdir(class_folder_path):\n",
    "        class_label = class_folder_name\n",
    "        images_per_class[class_label] = []\n",
    "        for image_path in glob.glob(os.path.join(class_folder_path, \"*.jpg\")):\n",
    "            img = cv2.imread(image_path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            #image_bgr = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "            images_per_class[class_label].append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in images_per_class.items():\n",
    "    print(\"{0} -> {1}\".format(key, len(value)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot images\n",
    "Plot images so we can see what the input looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for getting images class wise\n",
    "def plot_for_class(label):\n",
    "    nb_rows = 3\n",
    "    nb_cols = 3\n",
    "    fig, axs = plt.subplots(nb_rows, nb_cols, figsize=(6, 6))\n",
    "\n",
    "    n = 0\n",
    "    for i in range(0, nb_rows):\n",
    "        for j in range(0, nb_cols):\n",
    "            axs[i, j].xaxis.set_ticklabels([])\n",
    "            axs[i, j].yaxis.set_ticklabels([])\n",
    "            axs[i, j].imshow(images_per_class[label][n])\n",
    "            n += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_for_class(\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_for_class(\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_for_class(\"6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(images_per_class['0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Getting radom images from all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rn\n",
    "Z=images_per_class\n",
    "\n",
    "fig,ax=plt.subplots(5,5)\n",
    "fig.set_size_inches(15,15)\n",
    "for i in range(5):\n",
    "    for j in range (5):\n",
    "        l=rn.randint(0,len(Z)-1)\n",
    "        #print(l)\n",
    "        k=rn.randint(0,len(Z[str(l)])-1)\n",
    "        #print(k)\n",
    "        ax[i,j].imshow(Z[str(l)][k])\n",
    "        ax[i,j].set_title('Flower: '+str(l))\n",
    "        \n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From above groups of images we can observe there is huge variation in data set\n",
    "- There is a wide mix of similar feature among classes as well, like class 9,11,12 look a lot similar\n",
    "- If we notice Flowe class 2 in above image, flowers are very less compared to the bigger leaves it contains\n",
    "- Many of the images are center weighted and are good for details, but many of them has huge variations or very less presence of the image compared to overall image.\n",
    "- Color distribution is very dynamic among classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply different filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating functions for mask to observe the features\n",
    "- Reference:\n",
    "    - https://answers.opencv.org/question/134248/how-to-define-the-lower-and-upper-range-of-a-color/\n",
    "    - https://www.kaggle.com/nouradwai/plant-seedlings-fun-with-computer-vision\n",
    "    \n",
    "- Edge Detection masks were created below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = images_per_class[\"1\"][8]\n",
    "plt.imshow(image, cmap='gray')\n",
    "# 3x3 sobel filter for horizontal edge detection\n",
    "sobel_y = np.array([[ -1, -2, -1], \n",
    "                   [ 0, 0, 0], \n",
    "                   [ 1, 2, 1]])\n",
    "# vertical edge detection\n",
    "sobel_x = np.array([[-1, 0, 1],\n",
    "                   [-2, 0, 2],\n",
    "                   [-1, 0, 1]])\n",
    "# filter the image using filter2D(grayscale image, bit-depth, kernel)  \n",
    "filtered_image1 = cv2.filter2D(image, -1, sobel_x)\n",
    "filtered_image2 = cv2.filter2D(image, -1, sobel_y)\n",
    "f, ax = plt.subplots(1, 2, figsize=(15, 4))\n",
    "ax[0].set_title('horizontal edge detection')\n",
    "ax[0].imshow(filtered_image1, cmap='gray')\n",
    "ax[1].set_title('vertical edge detection')\n",
    "ax[1].imshow(filtered_image2, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = images_per_class[\"0\"][8]\n",
    "plt.imshow(image, cmap='gray')\n",
    "# 3x3 sobel filter for horizontal edge detection\n",
    "sobel_y = np.array([[ -1, -2, -1], \n",
    "                   [ 0, 0, 0], \n",
    "                   [ 1, 2, 1]])\n",
    "# vertical edge detection\n",
    "sobel_x = np.array([[-1, 0, 1],\n",
    "                   [-2, 0, 2],\n",
    "                   [-1, 0, 1]])\n",
    "# filter the image using filter2D(grayscale image, bit-depth, kernel)  \n",
    "filtered_image1 = cv2.filter2D(image, -1, sobel_x)\n",
    "filtered_image2 = cv2.filter2D(image, -1, sobel_y)\n",
    "f, ax = plt.subplots(1, 2, figsize=(15, 4))\n",
    "ax[0].set_title('horizontal edge detection')\n",
    "ax[0].imshow(filtered_image1, cmap='gray')\n",
    "ax[1].set_title('vertical edge detection')\n",
    "ax[1].imshow(filtered_image2, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = images_per_class[\"0\"][8]\n",
    "\n",
    "# 3x3 sobel filter for horizontal edge detection\n",
    "sobel_y = np.array([[ 0, -1, 0], \n",
    "                   [ -1, 5, -1], \n",
    "                   [ 0, -1, 0]])\n",
    "# vertical edge detection\n",
    "sobel_x = np.array([[ 0, -1, 0], \n",
    "                   [ -1, 5, -1], \n",
    "                   [ 0, -1, 0]])\n",
    "# filter the image using filter2D(grayscale image, bit-depth, kernel)  \n",
    "filtered_image1 = cv2.filter2D(image, -1, sobel_x,sobel_y)\n",
    "plt.imshow(filtered_image1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_for_plant(image):\n",
    "    image_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    sensitivity = 35\n",
    "    lower_hsv = np.array([60 - sensitivity, 100, 50])\n",
    "    upper_hsv = np.array([60 + sensitivity, 255, 255])\n",
    "\n",
    "    mask = cv2.inRange(image_hsv, lower_hsv, upper_hsv)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    return mask\n",
    "\n",
    "def segment_plant(image):\n",
    "    mask = create_mask_for_plant(image)\n",
    "    output = cv2.bitwise_and(image, image, mask = mask)\n",
    "    return output\n",
    "\n",
    "def sharpen_image(image):\n",
    "    image_blurred = cv2.GaussianBlur(image, (0, 0), 3)\n",
    "    image_sharp = cv2.addWeighted(image, 1.5, image_blurred, -0.5, 0)\n",
    "    return image_sharp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = images_per_class[\"9\"][65]\n",
    "\n",
    "image_mask = create_mask_for_plant(image)\n",
    "image_segmented = segment_plant(image)\n",
    "image_sharpen = sharpen_image(image_segmented)\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(20, 20))\n",
    "axs[0].imshow(image)\n",
    "axs[1].imshow(image_mask)\n",
    "axs[2].imshow(image_segmented)\n",
    "axs[3].imshow(image_sharpen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = images_per_class[\"0\"][65]\n",
    "\n",
    "image_mask = create_mask_for_plant(image)\n",
    "image_segmented = segment_plant(image)\n",
    "image_sharpen = sharpen_image(image_segmented)\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(20, 20))\n",
    "axs[0].imshow(image)\n",
    "axs[1].imshow(image_mask)\n",
    "axs[2].imshow(image_segmented)\n",
    "axs[3].imshow(image_sharpen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = images_per_class[\"1\"][8]\n",
    "\n",
    "image_mask = create_mask_for_plant(image)\n",
    "image_segmented = segment_plant(image)\n",
    "image_sharpen = sharpen_image(image_segmented)\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(20, 20))\n",
    "axs[0].imshow(image)\n",
    "axs[1].imshow(image_mask)\n",
    "axs[2].imshow(image_segmented)\n",
    "axs[3].imshow(image_sharpen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Importing data\n",
    "- Preprocessing the image for using it in models\n",
    "- For Supervised Models like KNN, \n",
    "    - importing and resizing it to 32x32 with RGB to maintain its colours and pattern (COLOR_BGR2RGB)\n",
    "    - For Interpolation using cv.INTER_AREA\n",
    "    - dividing the values with 255 to normalize it and make it float\n",
    "    - capturing the folder names as categories\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can not directly use the image, we have to process the image.\n",
    "\n",
    "from pathlib import Path\n",
    "from skimage.io import imread\n",
    "from keras.preprocessing import image\n",
    "import cv2 as cv\n",
    "def load_image_files(container_path):\n",
    "    image_dir = Path(container_path)\n",
    "    folders = [directory for directory in image_dir.iterdir() if directory.is_dir()]\n",
    "    categories = [fo.name for fo in folders]\n",
    "\n",
    "    images = []\n",
    "    flat_data = []\n",
    "    target = []\n",
    "    count = 0\n",
    "    train_img = []\n",
    "    label_img = []\n",
    "    for i, direc in enumerate(folders):\n",
    "        for file in direc.iterdir():\n",
    "            count += 1\n",
    "            img = imread(file)\n",
    "            #img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "            img_pred = cv.resize(img, (img_height, img_width), interpolation=cv.INTER_AREA)\n",
    "            img_pred = image.img_to_array(img_pred)\n",
    "            img_pred = img_pred / 255\n",
    "            train_img.append(img_pred)\n",
    "            label_img.append(categories[i])\n",
    "            \n",
    "    X = np.array(train_img)\n",
    "    y = np.array(label_img)\n",
    "    return X,y\n",
    "\n",
    "#Using the Keras pre-processing library the image is converted to an array and then normalised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "X,y = load_image_files(specPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exploring shape of imported data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring images captured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(X[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(X[1],cmap='gist_earth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(15,15))\n",
    "\n",
    "for i in range(1,101):\n",
    "  img=X[i]\n",
    "  fig.add_subplot(10,10,i)\n",
    "  plt.imshow(img,cmap='gray')\n",
    "\n",
    "plt.show()\n",
    "print('Label: ', y[1:101])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(15,15))\n",
    "\n",
    "for i in range(1,101):\n",
    "  img=X[1000+i]\n",
    "  fig.add_subplot(10,10,i)\n",
    "  plt.imshow(img)\n",
    "\n",
    "plt.show()\n",
    "print('Label: ', y[1001:1101])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image analysis \n",
    "- Images are not simple \n",
    "- Contains foreground and background details with multiple objects as noise in it like trees, big leaves, bees etc\n",
    "- Huge variations of data present\n",
    "- Images are not focused as well\n",
    "- Actual flowers are covering very less pixels compared to background and noise. This will lead to high class imbalance for target object and noise.\n",
    "- Simple supervised models will have hard time filtering the actual plants with soil and stones as they see the whole picture as a single input and are not splitting foreground from background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating data sets for training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Splitting Whole data set to Train, Val and Test with 80%, 10%, 10% respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, random_state=42, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View data set shape\n",
    "print(\"X_train: \"+str(X_train.shape))\n",
    "print(\"X_test: \"+str(X_test.shape))\n",
    "print(\"X_val: \"+str(X_val.shape))\n",
    "print(\"y_train: \"+str(y_train.shape))\n",
    "print(\"y_test: \"+str(y_test.shape))\n",
    "print(\"y_val: \"+str(y_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View Raw data in train set\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping Data sets for using in KNN model\n",
    "\n",
    "from builtins import range\n",
    "from builtins import object\n",
    "\n",
    "num_training = X_train.shape[0]\n",
    "mask = list(range(num_training))\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "num_test = X_test.shape[0]\n",
    "mask = list(range(num_test))\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]\n",
    "\n",
    "num_val = X_val.shape[0]\n",
    "mask = list(range(num_val))\n",
    "X_val = X_val[mask]\n",
    "y_val = y_val[mask]\n",
    "\n",
    "# Reshape the image data into rows\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "\n",
    "print(\"X_train: \"+str(X_train.shape))\n",
    "print(\"X_test: \"+str(X_test.shape))\n",
    "print(\"X_val: \"+str(X_val.shape))\n",
    "print(\"y_train: \"+str(y_train.shape))\n",
    "print(\"y_test: \"+str(y_test.shape))\n",
    "print(\"y_val: \"+str(y_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.view())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image classification with KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KNN\n",
    "- For Flower species raw data, expectations is the flower patern remain close to each other and a KNN model will be able pick up the common features and group it together\n",
    "- k-nearest neighbor algorithm is for classifying objects based on closest training examples in the feature space. k-nearest neighbor algorithm is among the simplest of all machine learning algorithms. Training process for this algorithm only consists of storing feature vectors and labels of the training images. In the classification process, the unlabelled query point is simply assigned to the label of its k nearest neighbors.\n",
    "- A main advantage of the KNN algorithm is that it performs well with multi-modal classes because the basis of its decision is based on a small neighborhood of similar objects. Therefore, even if the target class is multi-modal, the algorithm can still lead to good accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix,recall_score,f1_score,precision_score,roc_curve,log_loss,auc\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "#KNN Model with 1 neighbour\n",
    "\n",
    "KnnModel = KNeighborsClassifier(n_neighbors=1)\n",
    "KnnModel.fit(X_train,y_train)\n",
    "y_predict=KnnModel.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Accuracy score:',accuracy_score(y_test,y_predict))\n",
    "print('confuion matrix:\\n',confusion_matrix(y_test,y_predict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With single neighbour we are able to acheive close to 30% accuracy, but 1 neighbour is higly volatile and wont give us generalised result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the value of k and finding the accuracies on validation data\n",
    "k_vals = range(1, 30, 2)\n",
    "accuracies = []\n",
    "\n",
    "for k in range(1, 30, 2):\n",
    "  knn = KNeighborsClassifier(n_neighbors=k)\n",
    "  knn.fit(X_train, y_train)\n",
    "  score = knn.score(X_val, y_val)\n",
    "  print(\"k value=%d, accuracy score=%.2f%%\" % (k, score * 100))\n",
    "  accuracies.append(score)\n",
    " \n",
    "# finding the value of k which has the largest accuracy\n",
    "i = int(np.argmax(accuracies))\n",
    "print(\"k=%d value has highest accuracy of %.2f%% on validation data\" % (k_vals[i],accuracies[i] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Even though we got highest accuracies at 1 neighbour but we will go ahead with k=7 for more generalized approach which showed similar high accuracies on validation data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "knn.fit(X_train, y_train)\n",
    "predictions = knn.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"EVALUATION ON TESTING DATA\")\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(X_test[59].reshape(img_height,img_width,3))\n",
    "plt.show()\n",
    "image = X_test[59]\n",
    "print('Prediction:',knn.predict(image.reshape(1, -1)))\n",
    "print('Actual:',y_test[59])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(X_test[30].reshape(img_height,img_width,3))\n",
    "plt.show()\n",
    "image = X_test[30]\n",
    "print('Prediction:',knn.predict(image.reshape(1, -1)))\n",
    "print('Actual:',y_test[30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions = knn.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Accuracies from KNN are close to 23%.\n",
    "- We can observe on each classes precision and recall are very low.\n",
    "- Model is not able to identify and split relevent data from rest of the noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For flower species, expectations is the flower patern remain close to each other and a KNN model will be able pick up the common features and group it together.\n",
    "- While data analysis we noticed the images contains lot of noise or in few images actual flower is hardly cvering 5% of total pixels\n",
    "- Executed KNN from 1 to 30 neighbours and identified best values were at 1 neighbour and 25 neighbours. We chose for 7 neighbours so that we will get a more generalized prediction on classification.\n",
    "- With KNN our classification accuracies were close to 23% which are way below acceptable levels\n",
    "- Images with different angles are also getting mixed up with similar flower class creating a difference in prediction for KNN\n",
    "- We can observe there is an underlying pattern to the images for both raw pixel intensities and color. KNN is not capable enough to understand the differences and classifymore accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Issues with KNN\n",
    "- KNN depends on nearest neighbours, which might not be the best choice all the time. Observed the same issue during our evaluation proces as well\n",
    "- a major disadvantage of the KNN algorithm is that it uses all the features equally in computing for similarities. This can lead to classification errors, especially when there is only a small subset of features that are useful for classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image classification with Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, random_state=42, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using One hot encoder to convert the categories to array formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "one_hot_encoder.fit(y_train.reshape(-1, 1))\n",
    "\n",
    "y_train = one_hot_encoder.transform(y_train.reshape(-1, 1))\n",
    "#y_train = pd.DataFrame(data=y_train, columns=one_hot_encoder.categories_)\n",
    "\n",
    "y_test = one_hot_encoder.transform(y_test.reshape(-1, 1))\n",
    "#y_test = pd.DataFrame(data=y_test, columns=one_hot_encoder.categories_)\n",
    "\n",
    "y_val = one_hot_encoder.transform(y_val.reshape(-1, 1))\n",
    "#y_val = pd.DataFrame(data=y_val, columns=one_hot_encoder.categories_)\n",
    "\n",
    "\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "\n",
    "print(\"Shape of y_test:\", y_test.shape)\n",
    "\n",
    "print(\"Shape of y_val:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_cat = pd.DataFrame(data=y_test, columns=one_hot_encoder.categories_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Activation\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- created NN with less complexity as the results remained close and to reduce execution time as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1000,kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())                    \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dense(500,kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())                    \n",
    "model.add(Activation('relu'))  \n",
    "model.add(Dense(250,kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())                    \n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(125,kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())                    \n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(34,kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())                    \n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(17,kernel_initializer='he_normal'))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#updating learning rate\n",
    "adam = optimizers.Adam(lr=0.009, decay=1e-6)\n",
    "# Compile the model\n",
    "model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=\"adam\")\n",
    "\n",
    "# Fit the model\n",
    "history=model.fit(x=X_train, y=y_train, batch_size=batch_size, epochs=nb_epochs, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend(loc=0)\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can notice the train accuracies are increasing close to 100% but validation accuracies are around 40% only\n",
    "- We will try to augment the train data and train it again so that we will get less overfitted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test, y_test)\n",
    "print('Accuracy: %f ' % (results[1]*100))\n",
    "print('Loss: %f' % results[0])\n",
    "\n",
    "\n",
    "\n",
    "Y_pred_test_cls = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(X_test[10].reshape(img_height,img_width,3))\n",
    "plt.show()\n",
    "\n",
    "print('Label - one hot encoded: \\n',y_test_cat.iloc[10] )\n",
    "print('Actual Label - one hot encoded:  ', y_test[10])\n",
    "print('Predicted Label - one hot encoded: ',Y_pred_test_cls[10] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test data set accuracies are around 40%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With Neural network we were able to improve test set predictions to close to 40%.\n",
    "- Even with NN we are struggling for image identification with high accuracies.\n",
    "- Epochs were limited at 25 only as we will be comparing the same with CNN on similar grounds. We may acheive a bit higher accuracies with more epochs.\n",
    "- NN is able to provide better result compared to KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Classification with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential  # initial NN\n",
    "from tensorflow.keras.layers import Dense, Dropout # construct each layer\n",
    "from tensorflow.keras.layers import Conv2D # swipe across the image by 1\n",
    "from tensorflow.keras.layers import MaxPooling2D # swipe across by pool size\n",
    "from tensorflow.keras.layers import Flatten, GlobalAveragePooling2D\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, random_state=42, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "one_hot_encoder.fit(y_train.reshape(-1, 1))\n",
    "\n",
    "y_train = one_hot_encoder.transform(y_train.reshape(-1, 1))\n",
    "#y_train = pd.DataFrame(data=y_train, columns=one_hot_encoder.categories_)\n",
    "\n",
    "y_test = one_hot_encoder.transform(y_test.reshape(-1, 1))\n",
    "#y_test = pd.DataFrame(data=y_test, columns=one_hot_encoder.categories_)\n",
    "\n",
    "y_val = one_hot_encoder.transform(y_val.reshape(-1, 1))\n",
    "#y_val = pd.DataFrame(data=y_val, columns=one_hot_encoder.categories_)\n",
    "\n",
    "\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "\n",
    "print(\"Shape of y_test:\", y_test.shape)\n",
    "\n",
    "print(\"Shape of y_val:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_cat = pd.DataFrame(data=y_test, columns=one_hot_encoder.categories_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, (5, 5), activation='relu', input_shape=(img_height, img_width, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "\n",
    "\n",
    "\n",
    "model.add(Dense(17, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history=model.fit(x=X_train, y=y_train, batch_size=batch_size, epochs=nb_epochs, validation_data=(X_val, y_val))\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend(loc=0)\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With a very basic CNN model also we can see the validation accuracies are close to 52%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test, y_test)\n",
    "print('Accuracy: %f ' % (results[1]*100))\n",
    "print('Loss: %f' % results[0])\n",
    "\n",
    "\n",
    "\n",
    "Y_pred_test_cls = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(X_test[30].reshape(img_height,img_width,3))\n",
    "plt.show()\n",
    "\n",
    "print('Label - one hot encoded: \\n',y_test_cat.iloc[30] )\n",
    "print('Actual Label - one hot encoded:  ', y_test[30])\n",
    "print('Predicted Label - one hot encoded: ',Y_pred_test_cls[30] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adding few additional layers in model for it to capture more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(100, (5, 5), activation='relu', input_shape=(img_height, img_width, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(BatchNormalization()) \n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(BatchNormalization()) \n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(filters=256, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(17, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#updating learning rate\n",
    "adam = optimizers.Adam(lr=0.009, decay=1e-6)\n",
    "\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#Saving the best model using model checkpoint callback\n",
    "model_checkpoint=keras.callbacks.ModelCheckpoint('Flowerspecies_CNN_model.h5', #where to save the model\n",
    "                                                    save_best_only=True, \n",
    "                                                    monitor='val_accuracy', \n",
    "                                                    mode='max', \n",
    "                                                    verbose=1)\n",
    "\n",
    "history=model.fit(x=X_train, y=y_train, \n",
    "                  batch_size=batch_size, \n",
    "                  epochs=nb_epochs, \n",
    "                  validation_data=(X_val, y_val))\n",
    "                  #callbacks = [model_checkpoint])\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend(loc=0)\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can observe now the validation accuracies are increased closed to 60%\n",
    "- still there is a noticable gap betweeen train and validation accuracies, will try to close the gap in further models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test, y_test)\n",
    "print('Accuracy: %f ' % (results[1]*100))\n",
    "print('Loss: %f' % results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### - Test data set accuracies are immproved closed to 60%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "157MBWfwVORb"
   },
   "source": [
    "**Always save the model and its weights after training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WwvESxvvzRgz"
   },
   "outputs": [],
   "source": [
    "model.save('./Flower_Species_Classifier_CNN.h5')\n",
    "\n",
    "model.save_weights('./Flower_Species_Classifier_weights_CNN.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Usinng Image Generator to increase and improve the data set to add varations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen= keras.preprocessing.image.ImageDataGenerator(rotation_range=30,\n",
    "                                                      width_shift_range=0.3,\n",
    "                                                      height_shift_range=0.3,\n",
    "                                                      zoom_range=[0.4,1.5],\n",
    "                                                      horizontal_flip=True,\n",
    "                                                      vertical_flip=True)\n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(100, (5, 5), activation='relu', input_shape=(img_height, img_width, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(BatchNormalization()) \n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(BatchNormalization()) \n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(filters=256, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(17, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#updating learning rate\n",
    "adam = optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#Saving the best model using model checkpoint callback\n",
    "model_checkpoint=keras.callbacks.ModelCheckpoint('Flowerspecies_CNN_model.h5', #where to save the model\n",
    "                                                    save_best_only=True, \n",
    "                                                    monitor='val_accuracy', \n",
    "                                                    mode='max', \n",
    "                                                    verbose=1)\n",
    "\n",
    "\n",
    "history= model.fit_generator(datagen.flow(X_train, y_train, batch_size=32),  \n",
    "                  epochs=nb_epochs, \n",
    "                  validation_data=(X_val, y_val))\n",
    "                  #callbacks = [model_checkpoint])\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend(loc=0)\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test, y_test)\n",
    "print('Accuracy: %f ' % (results[1]*100))\n",
    "print('Loss: %f' % results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./Flower_Species_Classifier_CNN_Augmented.h5')\n",
    "\n",
    "model.save_weights('./Flower_Species_Classifier_weights_CNN_Augmented.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "history= model.fit_generator(datagen.flow(X_train, y_train, batch_size=32),  \n",
    "                  epochs=100, \n",
    "                  validation_data=(X_val, y_val))\n",
    "                  #callbacks = [model_checkpoint])\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend(loc=0)\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test, y_test)\n",
    "print('Accuracy: %f ' % (results[1]*100))\n",
    "print('Loss: %f' % results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./Flower_Species_Classifier_CNN_Augmented_100.h5')\n",
    "\n",
    "model.save_weights('./Flower_Species_Classifier_weights_CNN_Augmented_100.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With CNN we have noticed the simplest CNN is able to outperform NN.\n",
    "- Adding additional layers and using ImageGenerator for augmenting images gives a boost to the accuracies and it has increased the accuracies to 71%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reimporting Data and creatinng data set as jupyter notebook was crashing for whole execution in a single go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import VGG16\n",
    "#Load the VGG model\n",
    "vgg_conv = VGG16(weights='F:/GreatLearning/AI/ComputerVision/week 2/Week 2 - CV  - Mentor deck/Case study/data/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                 include_top=False,\n",
    "                 input_shape=(img_height, img_width, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re importing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Attempted to reimport data with 224x224x3 and do the model prep but my local system is not able to handle it and hence reduced it back to 100 again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can not directly use the image, we have to process the image.\n",
    "img_height=100\n",
    "img_width=100\n",
    "image_size=100\n",
    "specPath='F:\\\\GreatLearning\\AI\\\\ComputerVision\\\\Project\\\\Flowers-Classification\\\\17flowers-train\\\\jpg'\n",
    "\n",
    "\n",
    "#we can not directly use the image, we have to process the image.\n",
    "\n",
    "from pathlib import Path\n",
    "from skimage.io import imread\n",
    "from keras.preprocessing import image\n",
    "import cv2 as cv\n",
    "def load_image_files(container_path):\n",
    "    image_dir = Path(container_path)\n",
    "    folders = [directory for directory in image_dir.iterdir() if directory.is_dir()]\n",
    "    categories = [fo.name for fo in folders]\n",
    "\n",
    "    images = []\n",
    "    flat_data = []\n",
    "    target = []\n",
    "    count = 0\n",
    "    train_img = []\n",
    "    label_img = []\n",
    "    for i, direc in enumerate(folders):\n",
    "        for file in direc.iterdir():\n",
    "            count += 1\n",
    "            img = imread(file)\n",
    "            #img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "            img_pred = cv.resize(img, (img_height, img_width), interpolation=cv.INTER_AREA)\n",
    "            img_pred = image.img_to_array(img_pred)\n",
    "            #img_pred = img_pred / 255\n",
    "            train_img.append(img_pred)\n",
    "            label_img.append(categories[i])\n",
    "            \n",
    "    X = np.array(train_img)\n",
    "    y = np.array(label_img)\n",
    "    return X,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vggX = []\n",
    "vggy = []\n",
    "vggX,vggy = load_image_files(specPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exploring shape of imported data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vggX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vggy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vggy = np.asarray(vggy).reshape(vggy.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vggy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "one_hot_encoder.fit(vggy.reshape(-1, 1))\n",
    "\n",
    "vggy = one_hot_encoder.transform(vggy.reshape(-1, 1))\n",
    "\n",
    "print(\"Shape of y:\", vggy.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vggX_train, vggX_test,vggy_train, vggy_test = train_test_split(vggX, vggy, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vggX_val, vggX_test, vggy_val, vggy_test = train_test_split(vggX_test, vggy_test, random_state=42, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View data set shape\n",
    "print(\"X_train: \"+str(vggX_train.shape))\n",
    "print(\"X_test: \"+str(vggX_test.shape))\n",
    "print(\"X_val: \"+str(vggX_val.shape))\n",
    "print(\"y_train: \"+str(vggy_train.shape))\n",
    "print(\"y_test: \"+str(vggy_test.shape))\n",
    "print(\"y_val: \"+str(vggy_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,10)) # plot 25 images\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(vggX_train[i]/255, cmap=plt.cm.binary)\n",
    "    plt.xlabel(vggy_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use vgg16 pre-trained model with trainable densely connected output layer\n",
    "\n",
    "from keras.applications import VGG16\n",
    "#Load the VGG model\n",
    "#Loading VGG model from mentors deck as loading online was slowing things down\n",
    "#Code details captured from mentor deck\n",
    "\n",
    "\n",
    "vgg_conv = VGG16(weights='F:/GreatLearning/AI/ComputerVision/week 2/Week 2 - CV  - Mentor deck/Case study/data/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                 include_top=False,\n",
    "                 input_shape=(img_height, img_width, 3)\n",
    "                )\n",
    "\n",
    "# Freeze all the layers except for the last layer: \n",
    "for layer in vgg_conv.layers[:-4]:\n",
    "    layer.trainable = False\n",
    " \n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    " \n",
    "# Create the model\n",
    "model = models.Sequential()\n",
    " \n",
    "# Add the vgg convolutional base model\n",
    "model.add(vgg_conv)\n",
    " \n",
    "# Add new layers\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1024, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(17, activation='softmax'))\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image augmentation for train set and image resizing for validation & test\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator( \n",
    "      rescale=1./255,\n",
    "      rotation_range=20,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    " \n",
    "validation_datagen = ImageDataGenerator(rescale=1./255) \n",
    "\n",
    "train_batchsize = 100\n",
    "val_batchsize = 10\n",
    " \n",
    "train_generator = train_datagen.flow( \n",
    "        vggX_train,vggy_train,\n",
    "        batch_size=train_batchsize)\n",
    " \n",
    "validation_generator = validation_datagen.flow(\n",
    "        vggX_val,vggy_val,\n",
    "        batch_size=val_batchsize,\n",
    "        shuffle=False)\n",
    "\n",
    "test_generator = validation_datagen.flow(\n",
    "        vggX_test,vggy_test,\n",
    "        batch_size=val_batchsize,\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vggX_test=vggX_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=2e-4),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=vggX_train.shape[0]/train_generator.batch_size ,\n",
    "      epochs=nb_epochs,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=vggX_val.shape[0]/validation_generator.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend(loc=0)\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(vggX_test, vggy_test)\n",
    "print('Accuracy: %f ' % (results[1]*100))\n",
    "print('Loss: %f' % results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_test_cls = (model.predict(vggX_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(vggX_test[30]/255)\n",
    "plt.show()\n",
    "\n",
    "#print('Label - one hot encoded: \\n',vggy_test_cat.iloc[30] )\n",
    "print('Actual Label - one hot encoded:  ', vggy_test[30])\n",
    "print('Predicted Label - one hot encoded: ',Y_pred_test_cls[30] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./Flower_Species_Classifier_VGG16_Augmented_25.h5')\n",
    "\n",
    "model.save_weights('./Flower_Species_Classifier_weights_VGG16_Augmented_25.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With 25 epochs we are getting a accuracy of 80% which is close to CNN's result after 100 epochs\n",
    "- continue training the existing model for anothr 100 epochs and we can see immproved results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=vggX_train.shape[0]/train_generator.batch_size ,\n",
    "      epochs=100,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=vggX_val.shape[0]/validation_generator.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend(loc=0)\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(vggX_test, vggy_test)\n",
    "print('Accuracy: %f ' % (results[1]*100))\n",
    "print('Loss: %f' % results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_test_cls = (model.predict(vggX_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(vggX_test[30]/255)\n",
    "plt.show()\n",
    "\n",
    "#print('Label - one hot encoded: \\n',vggy_test_cat.iloc[30] )\n",
    "print('Actual Label - one hot encoded:  ', vggy_test[30])\n",
    "print('Predicted Label - one hot encoded: ',Y_pred_test_cls[30] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With Pretrained Model we can observe the training time is reduced and accuracies are increaed to 90%\n",
    "- CNN Models were better compared to NN as they are able to cpature details of foreground and background better and able to classify things.\n",
    "- Our Accuracies has improved from 20% in KNN ===> 40% in NN ===> 70% in CNN ===> 90% in Pretrained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Model using Keras and Pickel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./Flower_Species_Classifier_VGG16_Augmented_100.h5')\n",
    "\n",
    "model.save_weights('./Flower_Species_Classifier_weights_VGG16_Augmented_100.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keras suggests to use model.save and model.save_weights but we can pickle model using joblib as well for bigger model if usual pickling fails\n",
    "from sklearn.externals import joblib \n",
    "  \n",
    "# Save the model as a pickle in a file \n",
    "joblib.dump(model, 'Flower_Species_Classifier_VGG16_Augmented_100.pkl') \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the file \n",
    "model_joblib = joblib.load('Flower_Species_Classifier_VGG16_Augmented_100.pkl')  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating code to classify single image for UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting single data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting single image to expected format for model prediction\n",
    "pred_x = np.expand_dims(vggX_test[30], axis=0)\n",
    "pred_x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the loaded model to make predictions \n",
    "#model_joblib.predict(X_test)\n",
    "Y_pred_test_cls = (model_joblib.predict(pred_x) > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(pred_x.reshape(img_height,img_width,3)/255)\n",
    "plt.show()\n",
    "\n",
    "#print('Label - one hot encoded: \\n',vggy_test_cat.iloc[30] )\n",
    "print('Actual Label - one hot encoded:  ', vggy_test[30])\n",
    "print('Predicted Label - one hot encoded: ',Y_pred_test_cls[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoder.inverse_transform(Y_pred_test_cls[0].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumping the transformer to an external pickle file\n",
    "joblib.dump(one_hot_encoder, 'VGG16_CNN_ohe.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting from Pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_model=joblib.load('./Flower_Species_Classifier_VGG16_Augmented_100.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the loaded model to make predictions \n",
    "#model_joblib.predict(X_test)\n",
    "Y_pred_test_cls = (pkl_model.predict(pred_x) > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(pred_x.reshape(img_height,img_width,3)/255)\n",
    "plt.show()\n",
    "\n",
    "#print('Label - one hot encoded: \\n',vggy_test_cat.iloc[30] )\n",
    "print('Actual Label - one hot encoded:  ', vggy_test[30])\n",
    "print('Predicted Label - one hot encoded: ',Y_pred_test_cls[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading ppickeled encoder\n",
    "ohe_pkl=joblib.load('./VGG16_CNN_ohe.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_pkl.inverse_transform(Y_pred_test_cls[0].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictinng from Images directly from directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageTk, Image\n",
    "import numpy as np\n",
    "#from tkinter import filedialog\n",
    "#import tkinter as tk\n",
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from sklearn.externals import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def predict_img(image_data):\n",
    "    #root = tk.Tk()\n",
    "    #image_data = filedialog.askopenfilename(initialdir=\"/\", title=\"Choose an image\",\n",
    "    #                                   filetypes=((\"all files\", \"*.*\"), (\"jpg files\", \"*.jpg\"), (\"png files\", \"*.png\")))\n",
    "    \n",
    "    original = Image.open(image_data)\n",
    "    plt.figure(figsize = (5,5))\n",
    "    plt.imshow(original)\n",
    "    original = original.resize((100, 100), Image.ANTIALIAS)\n",
    "    numpy_image = img_to_array(original)\n",
    "    \n",
    "    #expanding dimensions as model is expecting a array of image not just a single image\n",
    "    image_batch = np.expand_dims(numpy_image, axis=0)\n",
    "    processed_image=image_batch/255\n",
    "    \n",
    "    #Loading Pickled Model , we could have used Keras approach as well but we are going ahead with Pickle for now\n",
    "    vgg_cnn_model =joblib.load('./Flower_Species_Classifier_VGG16_Augmented_100.pkl')\n",
    "\n",
    "    #Loading pickeled encoder for reverse transforming output\n",
    "    ohe_pkl=joblib.load('./VGG16_CNN_ohe.pkl')\n",
    "    \n",
    "    #Using the pickeled model for classification\n",
    "    predictions = vgg_cnn_model.predict(processed_image)\n",
    "    \n",
    "    #inverse transforming the prediction for folder name details\n",
    "    label = ohe_pkl.inverse_transform(predictions.reshape(1,-1))\n",
    "    print(\"Predicted label for the Image: \",label)\n",
    "    #root.quit()\n",
    "    #root.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test path -->> F:\\GreatLearning\\AI\\ComputerVision\\Project\\Flowers-Classification\\Test\\0.jpg\n",
    "predict_img('F:\\\\GreatLearning\\\\AI\\\\ComputerVision\\\\Project\\\\Flowers-Classification\\\\Test\\\\0.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_img('F:\\\\GreatLearning\\\\AI\\\\ComputerVision\\\\Project\\\\Flowers-Classification\\\\Test\\\\1.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating separate jupyter notebook for UI to keep things modular & validate the pickle files can cork independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - For a detailed observations we can refer my existing details on ML Models/NN/CNN variations and we can add the benefits that a pretrained model is bringing to the picture- Copied below from Part 2 of project\n",
    " - we have to divide the models into two major groups:\n",
    "     - Machine learning Algorithms\n",
    "     - Deep Neural Networks (NN & CNN)\n",
    "\n",
    " - Machine Learning Algorithms:\n",
    "     - these algo's learn their mapping from provided input and output i.e. alog learns a function with diff sets of weight which help in predicting the accurate values.\n",
    "     - For classification algorithms they learns from the term being used \"Decision Boundries\"\n",
    "     - These decision boundries determine wether a new point belongs to which class or groups\n",
    "     - Decision boundries could vary from linear to non-linear and these algo's are very strong to identify any relationship and map it with proper function and weight.\n",
    "     - Image classification althoug a classification problem but it has much more details and relationships which ML Decision boundries are not able to map and replicate without very high computation and some times even thats not enough and becomme impossible for usinng these algos.\n",
    "     \n",
    " - Deep Neural Netowrks ( ANN & CNN ):\n",
    "     - Deep Neural networks brought in a different concept called Feature Engineering\n",
    "         - Feature Extraction\n",
    "         - Feature Selection\n",
    "     - In feature extraction, we extract all the required features for our problem statement\n",
    "     - In feature selection, we select the important features that improve the performance of our deep learning model.\n",
    "     - By this design change in model is giving us huge advantage over Machine learning algorithms for identifying important features of the image and relationships with outputs which helps in categorizing/predictiong classes more accurately.\n",
    "     \n",
    " - Chalanges for Neural Network:\n",
    "     - NN amount of weight become unmanagable becuse it uses one perceptron for each input/pixel\n",
    "     - too many parameters as its fully connected\n",
    "     - each node is connected to previous and next layer making it very dense and many connections are redundant\n",
    "     - Translation Invariant - NN behaves differently to shifted version of same image/zoomed/inverted. To make it learn all those you have to feed all varaions of data, which is higly difficult.\n",
    "     - NN expects an identified object should appear on that specific place only which is never the real world scenario.\n",
    "     - spatial information is lost when the image is flattened(matrix to vector)\n",
    "     - This will make image processing difficult as the model will tend to overfit and capture unnecessary relationship\n",
    "     \n",
    " - CNN Advantages:\n",
    "\n",
    "    - Convolution:\n",
    "         - feed forward NN wont see any order in their inputs\n",
    "         - CNN on other hand is better at dealing with multiple kinds of spatial deformations. It take advantage of local spatial coherence of images. \n",
    "         - This means that they are able to reduce dramatically the number of operation needed to process an image by using convolution on patches of adjacent pixels, because adjacent pixels together are meaningful. \n",
    "         - We also call that local connectivity.\n",
    "         - convolution in neural networks is operation of finding patterns. It has kernel that with which it basically scan an image and place where kernel have 100% match is a place where pattern matched. \n",
    "    - Pooling layers:\n",
    "        - downscale the image\n",
    "        - This is possible because we retain throughout the network, features that are organized spatially like an image, and thus downscaling them makes sense as reducing the size of the image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretrained Models observations compared with DNN/CNN\n",
    "- Deep Nueral networks:\n",
    "    - A DNN works very well for classification and regression but it may not perform well with image classification, as we noticed in our models as well\n",
    "    - We were not able to improve the model performance significantly even after working with multiple factors like\n",
    "        - adding more layers\n",
    "        - diff optimizers\n",
    "        - no. of epochs\n",
    "        - amount of data\n",
    "- Convolutional Neural Network:\n",
    "    - Convolution layers are very successful in tasks involving images classification, object identification, face recognition etc. \n",
    "    - They allow parameter sharing which results in a very optimized network compared to using Dense layers. This reduces the model commplexity as well the training time.\n",
    "- Transfer Learning:\n",
    "    - \"Transfer learning is a machine learning method where a model developed for a task is reused as the starting point for a model on a second task.\"\n",
    "    - \"In transfer learning, we first train a base network on a base dataset and task, and then we repurpose the learned features, or transfer them, to a second target network to be trained on a target dataset and task. This process will tend to work if the features are general, meaning suitable to both base and target tasks, instead of specific to the base task.\"\n",
    "    - idea is to use a state of the art model which is already trained on a larger dataset for long time and proven to work well in related task\n",
    "    - using transfer learning we could train a model which have Test set accuracy of 90% in only 12 mins which is much better when compared to earlier models.\n",
    "    - We can further increase the accuracy by using\n",
    "        - More data/augmentation\n",
    "        - More epochs/training steps\n",
    "        - Adding more layers\n",
    "        - More regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reference\n",
    "    - https://www.analyticsvidhya.com/blog/2020/08/top-4-pre-trained-models-for-image-classification-with-python-code/\n",
    "    - https://machinelearningmastery.com/transfer-learning-for-deep-learning/#:~:text=Transfer%20learning%20is%20a%20machine,model%20on%20a%20second%20task.&text=Common%20examples%20of%20transfer%20learning,your%20own%20predictive%20modeling%20problems.\n",
    "    - https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
